{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/fms-extras/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch._inductor.config\n",
    "from fms.models import get_model\n",
    "from fms.utils import generation, tokenizers\n",
    "from torch import distributed as dist\n",
    "\n",
    "import fms_extras.models.paged_gpt_bigcode\n",
    "import fms_extras.models.paged_llama\n",
    "from fms_extras.models.speculator import MLPSpeculator\n",
    "from fms_extras.utils.generation import paged_generate, speculative_generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function fms.models.get_model(architecture: str, variant: str, model_path: Optional[str] = None, source: Optional[str] = None, device_type: str = 'cpu', distributed_strategy: Optional[str] = None, checkpoint_sharding: Optional[str] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, **kwargs)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fms.models import get_model\n",
    "\n",
    "\n",
    "get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture='paged_llama', variant='llama3.8b', model_path='/models/Meta-Llama-3-8B-Instruct', source='hf', device_type='cuda', distributed_strategy=None, checkpoint_sharding=None, group=None, kwargs={}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PagedLLaMA(\n",
       "  (headless_model): PagedLLaMAHeadless(\n",
       "    (shared): WordEmbedding(\n",
       "      (emb): Embedding(128256, 4096)\n",
       "      (head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PagedLLaMABlock(\n",
       "        (ln): LayerNormParameterized()\n",
       "        (ff_ln): LayerNormParameterized()\n",
       "        (attn): PagedMultiHeadAttention(\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (qkv_fused): Linear(in_features=4096, out_features=6144, bias=False)\n",
       "        )\n",
       "        (ff_sub_layer): GatedLinearUnit(\n",
       "          (wg1_fused): Linear(in_features=4096, out_features=28672, bias=False)\n",
       "          (a): SiLU()\n",
       "          (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dec_norm): LayerNormParameterized()\n",
       "  )\n",
       "  (head): WordEmbedding(\n",
       "    (emb): Embedding(128256, 4096)\n",
       "    (head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#from torch import distributed as dist\n",
    "\n",
    "import fms_extras.models.paged_gpt_bigcode\n",
    "import fms_extras.models.paged_llama\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.half)\n",
    "\n",
    "model = get_model(\n",
    "    f\"paged_llama\",\n",
    "    'llama3.8b',\n",
    "    model_path='/models/Meta-Llama-3-8B-Instruct',\n",
    "    checkpoint_sharding=None,\n",
    "    device_type=device,\n",
    "    source='hf',\n",
    "    distributed_strategy=None,\n",
    "    group=None,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7bf4515da830>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from fms.utils import generation, tokenizers\n",
    "\n",
    "\n",
    "tokenizer = tokenizers.get_tokenizer('/models/Meta-Llama-3-8B-Instruct')\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPSpeculator(\n",
       "  (emb): ModuleList(\n",
       "    (0-3): 4 x Embedding(128256, 3072)\n",
       "  )\n",
       "  (proj): ModuleList(\n",
       "    (0): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "    (1-3): 3 x Linear(in_features=3072, out_features=3072, bias=False)\n",
       "  )\n",
       "  (head): ModuleList(\n",
       "    (0-3): 4 x Linear(in_features=3072, out_features=128256, bias=False)\n",
       "  )\n",
       "  (ln): ModuleList(\n",
       "    (0-3): 4 x LayerNormParameterized()\n",
       "  )\n",
       "  (activation): GELU(approximate='none')\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fms_extras.models.hf.modeling_mlp_speculator import MLPSpeculatorPreTrainedModel\n",
    "\n",
    "\n",
    "speculator = MLPSpeculatorPreTrainedModel.from_pretrained(\n",
    "    'ibm-fms/llama3-8b-accelerator', device_map=device\n",
    ").speculator\n",
    "speculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speculator.n_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.nheads, model.config.kvheads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_gpu_memory=25386352640, gpu_memory_utilization=0.96, peak_memory=22458458112, cache_block_size=8388608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fms_extras.utils.cache.paged.PagedKVCacheManager at 0x7bf4515d9de0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fms_extras.utils.cache.paged import PagedKVCacheManager\n",
    "\n",
    "\n",
    "kv_cache_manager = PagedKVCacheManager(\n",
    "    model.config.nlayers,\n",
    "    model.config.nheads,\n",
    "    model.config.emb_dim,\n",
    "    kv_heads=model.config.kvheads,\n",
    "    tensor_parallel_size=1,\n",
    "    dtype=torch.get_default_dtype(),\n",
    "    device=device,\n",
    ")\n",
    "kv_cache_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id != tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nProvide a list of instructions for preparing chicken soup.\\n\\n### Response:'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{}\\n\\n### Response:\"\n",
    "\n",
    "prompt1 = template.format(\n",
    "    \"Provide a list of instructions for preparing chicken soup.\"\n",
    ")\n",
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000,  39314,    374,    459,   7754,    430,  16964,    264,   3465,\n",
       "            13,   9842,    264,   2077,    430,  36001,  45695,    279,   1715,\n",
       "           382,  14711,  30151,    512,  61524,    264,   1160,    315,  11470,\n",
       "           369,  20646,  16553,  19724,    382,  14711,   6075,     25],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ids_for_prompt(prompt):\n",
    "    tokens = tokenizer.tokenize(prompt)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    ids = [tokenizer.bos_token_id] + ids\n",
    "    ids = torch.tensor(ids, dtype=torch.long, device=device)\n",
    "    return ids\n",
    "\n",
    "prompt1 = ids_for_prompt(prompt1)\n",
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [prompt1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cudagraphs = True\n",
    "max_seq_len = (\n",
    "    model.config.max_expected_seq_len\n",
    "    if hasattr(model.config, \"max_expected_seq_len\")\n",
    "    else model.config.max_pos\n",
    ")\n",
    "max_seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result, n_steps, ttft, generated_token_time_out = paged_generate(\n",
    "#    model,\n",
    "#    ids,\n",
    "#    kv_cache_manager,\n",
    "#    max_new_tokens=100,\n",
    "#    max_seq_len=max_seq_len,\n",
    "#    do_sample=False,\n",
    "#    decode_model=None,\n",
    "#    cudagraphs=cudagraphs,\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([128000,  39314,    374,    459,   7754,    430,  16964,    264,   3465,\n",
       "              13,   9842,    264,   2077,    430,  36001,  45695,    279,   1715,\n",
       "             382,  14711,  30151,    512,  61524,    264,   1160,    315,  11470,\n",
       "             369,  20646,  16553,  19724,    382,  14711,   6075,     25,   4815,\n",
       "            8586,    374,    264,   1160,    315,  11470,    369,  20646,  16553,\n",
       "           19724,   1473,    334,   8468,    220,     16,     25,  50095,  52275,\n",
       "           57277,      9,    220,     16,  31123,  17685,   1752,     11,   6930,\n",
       "            1752,  16553,  17659,    477,  60611,    198,      9,    220,     19,\n",
       "           26446,  16553,  45993,    198,      9,    220,     16,   3544,  38427,\n",
       "              11,  38525,    198,      9,    220,     18,  85388,  31735,     11,\n",
       "           94927,    198,      9,    220,     17,  62517,     11,  83612,    323,\n",
       "           38525,    198,      9,    220,     17,  70121,  55972,     82,     11,\n",
       "           38525,    198,      9,    220,     16,  42384,  32720,    270,  31218,\n",
       "             198,      9,  28138,    323,  25349,     11,    311,  12945,    271,\n",
       "             334,   8468,    220,     17,     25,  32266,    279,  34619,  57277,\n",
       "               9], device='cuda:0')],\n",
       " 35,\n",
       " 0.33524250984191895,\n",
       " 2.1397411823272705,\n",
       " [3,\n",
       "  3,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fms_extras.utils.generation import paged_generate, speculative_generate\n",
    "\n",
    "\n",
    "result, n_steps, ttft, generated_token_time_out, n_accepts = speculative_generate(\n",
    "    model,\n",
    "    ids,\n",
    "    speculator,\n",
    "    kv_cache_manager,\n",
    "    new_tokens=100,\n",
    "    max_seq_len=max_seq_len,\n",
    "    decode_model=None,\n",
    "    # todo: we can only reduce-overhead for now when batch size is 1\n",
    "    flattening=True,\n",
    "    cudagraphs=False,\n",
    "    threshes=[4,3,2,2],\n",
    ")\n",
    "\n",
    "result, n_steps, ttft, generated_token_time_out, n_accepts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
